% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{longtable}
\usepackage{float} % For precise float placement
\usepackage{subcaption} % For subtables
\usepackage{afterpage} % For forcing table to appear after current page
\usepackage{setspace} % For adjusting line spacing
\usepackage{dialogue}
\usepackage{xcolor}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}
% Used for a dialog representation

   
\begin{document}
%
\title{Conversational Task Agent}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Guilherme Fernandes\inst{1}\orcidID{60045} \and
Ricardo Silva\inst{1}\orcidID{60559} \and
Vladyslav Mikytiv\inst{1}\orcidID{60735}}
%
\authorrunning{Guilherme, Ricardo, Vladyslav}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{NOVA School Of Science and Technology}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
Our project focuses on creating an adaptive taskbot tailored specifically for guiding users through cooking tasks. This taskbot will utilize artificial intelligence and natural language processing to interact with users, understanding their progress and constraints in real-time. The taskbot will dynamically adjust instructions based on the ingredients and cooking tools available to the user. In instances where users encounter obstacles, such as running out of ingredients or lacking specific utensils, the taskbot will intelligently adapt the recipe, offering alternative ingredient substitutions or equipment alternatives to ensure successful completion of the dish. By providing personalised guidance our taskbot aims to enhance the culinary experience.

\keywords{Taskbot  \and Word embedding \and Natural Language Processing}
\end{abstract}
%
%
%
\section{Introduction}
In recent years, word embedding has emerged as a cornerstone technology in the field of artificial intelligence, revolutionising natural language processing (NLP) tasks. This transformative approach to representing words as dense vectors in continuous vector spaces has garnered significant attention and acclaim within the AI community. The rise of word embedding has been marked by its progression from conventional methods to the creation of sophisticated models capable of capturing intricate semantic connections among words.

The inception of word embedding can be traced back to the early 2000s when researchers began exploring methods to overcome the limitations of traditional vector space models, such as bag-of-words and one-hot encoding. These initial efforts paved the way for more advanced approaches that would later reshape the field of natural language processing (NLP).

However, it wasn't until the introduction of groundbreaking techniques like Word2Vec by Mikolov et al. in 2013 and GloVe (Global Vectors for Word Representation) by Pennington et al. in 2014 that word embedding started to gain widespread attention and adoption. These models introduced novel methodologies for learning distributed representations of words based on large corpora of text, capturing intricate semantic nuances and relationships.

The allure of word embedding lies in its ability to encode semantic meaning into dense vector representations, enabling machines to understand language in a more nuanced and contextually rich manner.

The project aims to utilize embedding algorithms and large language models to develop a dialog manager between users and the taskbot. This manager will execute user requests and guide them through processes, particularly in cooking recipes, with adaptability. Leveraging a dataset containing recipes data, including ingredients, procedural steps, and difficulty levels, we utilize OpenSearch indexing to efficiently retrieve information for user queries.


\section{Algorithms and Implementation}
\subsection{Indexing}
We must implement a search index of complex manual tasks in order to allow our system to retrieve relevant information about the tasks.

\begin{figure}[!htbp]
    \center
    \includegraphics[scale=0.08]{images/task_retriever.jpg}
    \caption{Task retrieval}
\end{figure}

We must parse the data from the dataset which is a JSON file. We do that while we populate our indexes in OpenSearch. For each recipe we parse the JSON and send the relevant information to the index mapping that was created. We introduce a set of mappings tailored to effectively index recipe data and they are the following:

    
\begin{table}[ht]
\centering
\setlength{\abovecaptionskip}{10pt} % Adjust the spacing here
\resizebox{0.7\textwidth}{!}{%
\begin{subtable}{0.45\linewidth}
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Field} & \textbf{Data Type} \\ \hline
recipeJson & object \\ \hline
recipeName & text \\ \hline
prepTimeMinutes & integer \\ \hline
cookTimeMinutes & integer \\ \hline
totalTimeMinutes & integer \\ \hline
difficultyLevel & keyword \\ \hline
images & array of text \\ \hline
servings & float \\ \hline
\end{tabular}
\end{subtable}
\hfill % Add horizontal space between subtables
\begin{subtable}{0.45\linewidth}
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Field} & \textbf{Data Type} \\ \hline
videos & array of object \\ \hline
tools & array of text \\ \hline
cuisines & array of text \\ \hline
courses & array of text \\ \hline
diets & array of text \\ \hline
ingredients & array of object \\ \hline
stepsEmbedding & knn\_vector \\ \hline
\end{tabular}
\end{subtable}
}
\caption{Index Mapping}
\label{tab:mappings}
\end{table}

An important detail to emphasise is that our \textbf{ingredients} are a \textbf{nested type} with the properties \textbf{name} and most importantly the
corresponding \textbf{\text{ingredient\_embedding}}. We also used the mpnet-base-v2 transformer because it had better results.~\cite{mpnet}.

\subsection{Searching}
We've initiated the project by focusing on leveraging indexes for efficient dataset navigation, and OpenSearch emerges as a pivotal tool for executing queries seamlessly.

We've commenced with text-based queries, enabling users to access recipes based on various parameters such as cooking time, difficulty level, course type, or serving size.

Moreover, the integration of boolean queries adds another layer of flexibility, empowering users to refine their searches by specifying ingredients they desire or wish to avoid in their recipes.

Embedded queries stand out as particularly intriguing among our query types. By transforming user inputs into vector representations, we can uncover recipes closely related to the query. This method offers personalised recipe recommendations that align with users' interests more effectively.

In our index mappings, we utilize various embeddings to facilitate our searches. A crucial component is a model, referenced as~\cite{bert}, which extracts relevant information from natural language phrases, particularly focusing on ingredient extraction from user queries. This model also aids in processing the textual content within recipe steps.

Acknowledging the significance of recipe steps, we've introduced embeddings for both the recipe names and their corresponding individual steps. This approach enables our search system to capture the essence of each recipe efficiently, ensuring accurate query results.

After thorough testing, we've identified the most effective embeddings for our system: ingredient embeddings for individual ingredients and step embeddings paired with respective recipe names. Despite transformer limitations, these embeddings show promising results, especially in user-input based embedded queries. Given that our system doesn't prioritize contextual nuances in phrases, we've opted to utilize FoodBERT~\cite{bert} to maintain consistency with this approach.

In essence, our framework enables a dynamic exploration of the dataset, granting users the ability to refine their searches precisely according to their preferences and specific requirements.

\subsection{Vision Models}
Our system currently excels in recipe searches, but what about images? Incorporating image queries would greatly enhance its functionality. To achieve this, we require a vision model like CLIP~\cite{clip} that enables us to seamlessly integrate image searches into our system. CLIP is a vision model encoder that facilitates the computation of semantic similarity between a sentence and an image.

CLIP features a dual encoder architecture comprising of an \textbf{image encoder}, responsible for generating embedding vectors for images, and the \textbf{text encoder}, which produces embedding vectors for text. Notably, the text encoder's embeddings are contextualized within the realm of images, offering representations of text within an image context. This design enables comparisons between text and images, images and text, as well as images and other images.

To enable this functionality, we've revised our OpenSearch index mapping to accommodate these types of embeddings. Specifically, we've introduced a new embedding that combines the recipe name with its corresponding image. As a result, our index mapping table has been updated as follows:

\begin{table}[ht]
\centering
\setlength{\abovecaptionskip}{10pt} % Adjust the spacing here
\resizebox{0.7\textwidth}{!}{%
\begin{subtable}{0.45\linewidth}
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Field} & \textbf{Data Type} \\ \hline
recipeJson & object \\ \hline
recipeName & text \\ \hline
prepTimeMinutes & integer \\ \hline
cookTimeMinutes & integer \\ \hline
totalTimeMinutes & integer \\ \hline
difficultyLevel & keyword \\ \hline
images & array of text \\ \hline
servings & float \\ \hline
\end{tabular}
\end{subtable}
\hfill % Add horizontal space between subtables
\begin{subtable}{0.45\linewidth}
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Field} & \textbf{Data Type} \\ \hline
videos & array of object \\ \hline
tools & array of text \\ \hline
cuisines & array of text \\ \hline
courses & array of text \\ \hline
diets & array of text \\ \hline
ingredients & array of object \\ \hline
stepsEmbedding & knn\_vector \\ \hline
\bfseries\colorbox{yellow}{imageEmbedding} & knn\_vector \\ \hline
\end{tabular}
\end{subtable}
}
\caption{Updated Index Mapping}
\label{tab:mappings}
\end{table}

These embeddings incorporate the recipe title, not just the raw image, we observed improved query responses with this approach. To implement this, we've developed a KNN search mechanism that accepts either an image or text input. Initially, it performs a KNN search on the title embedding, retrieving the top results. Subsequently, we mix these results with another KNN search, which operates on the image embedding. This allows us to get the best results from both KNN searches. Since CLIP performs better with natural language, we enhance the embedding of recipe titles by prefixing the text with \textbf{"A photo of"}. This addition creates a more natural and descriptive phrase, aligning with the contextual understanding of CLIP's model.

With these new embeddings, users can now input either an image or text. For image inputs, we search in the embedding space to find recipes with the highest similarity scores. Similarly, text inputs are encoded using CLIP to compare with image embeddings, enabling retrieval of recipes whose images closely match the user's initial text query. 


Given that we store this data in OpenSearch, performing a KNN search also returns the images associated with the recipes. This architecture allows us to retrieve images not only from image queries but also from text queries. As a result, we can seamlessly access images corresponding to the search query, whether it's a text input or an image input.

It's worth highlighting that the original embeddings are distinct from the newly generated ones via CLIP. These CLIP text embeddings are specifically tailored for integration within an image-oriented context. 

We've significantly expanded our capabilities for returning recipes, starting from basic queries and progressing to more complex ones. Initially, we utilize simple queries and a straightforward embedding method for KNN searches. With the recent integration of CLIP, our capabilities have expanded further. Now, we can conduct image queries and text queries within the context of images, leveraging CLIP's advanced features for enhanced accuracy and relevance.

\subsection{Large Language Model: PlanLLM}
After retrieving a recipe, we will want to work with the specific recipe the user selected. For that we will use the aid of a large language model. PlanLLM is a conversational assistant trained to assist users in completing a recipe from beginning to end and be able to answer any related or relevant requests that the user might have ~\cite{planllmeacl24}

To integrate PlanLLM into our project, we need to manage a JSON file, which is then sent to a PlanLLM API. This API returns the PlanLLM's response based on the user's query. The manipulation and updating of the JSON file are managed by our system. When a user selects a recipe, we narrow our focus to the context of that specific recipe, guiding the user through the steps needed to reach the end goal.

The user can talk with the PlanLLM and receive answers such as:\\[5pt]

\hspace{0.5cm} % Adjust the value as needed for the desired horizontal shift
\begin{minipage}{1.2\textwidth} % Adjust width as needed
\begin{dialogue}
\speak{User} Let's begin the task! 
\speak{PlanLLM} Hi! How can I assist you today? 
\speak{User} What do I need to do?
\speak{PlanLLM} You are in the first step, preheat the oven to 350 degress.
\speak{User} Done! Let's move to the next step.
\speak{PlanLLM} Step 2: Mix the ingredients together.
\speak{User} In which step are we?
\speak{PlanLLM} We currently on Step 2: Mix the ingredients together.\\[5pt]
\end{dialogue}
\end{minipage}

We want to allow the user to take a photo of what he's currently doing, such as cutting tomatoes for a recipe, and we want the PlannLLM to figure out which step we're at and what we're doing. And since the PlanLLM isn't trained to directly process images we implemented an intermediate processing step to handle such cases. This involves transforming user-input images into text representations using CLIP. By comparing the embeddings of the image with the textual descriptions of each recipe step, we can determine which step the image corresponds to. This intermediary step ensures that images can be effectively incorporated into the recipe guidance process.

\begin{figure}[!htbp]
    \center
    \includegraphics[scale=0.08]{images/planLLM.jpg}
    \caption{Image passing to PlanLLM}
\end{figure}

In order to re-create a conversation like the one above we need to carefully manipulate the JSON to get the answers we want. We've observed that PlanLLM struggles with transitioning between non-consecutive steps, such as starting from step 5 instead of step 1. However, the model responds effectively when the user provides a precise instruction like \textbf{"I am currently on step x"}. In such cases, we update the internal state to reflect the specified step provided by the user.

To achieve a more fluid and natural conversation flow, we've implemented a mechanism that enables users to provide either text or images as input. Using CLIP, we predict the step in the recipe that the user is referring to by comparing embedding scores and identifying the most relevant match. This approach allows users to jump between steps as desired through a simple photo of what they're currently doing, without the constraint of needing to use specific phrases like \textbf{"I am currently on step x"}. With this functionality in place, the remaining task is to orchestrate the entire dialogue with the user, which we'll cover in the next chapter.

\subsection{Contextual Embedding and Self-Attention Analysis}
\subsubsection{Contextual Embeddings:}
Let's analyse the contextual embedding of two phrases and look at the evolution in layers.

\begin{figure}[!htbp]
    \center
    \includegraphics[scale=0.2]{images/contextual\_embeddings}
    \caption{Contextual embeddings over layers}
\end{figure}
As we observe the evolution of the model's layers, we notice that tokens closely related to each other tend to cluster together, distinct from other tokens. With deeper layers, the model becomes adept at capturing intricate and abstract semantic relationships among words and phrases. This capability enables it to form more precise clusters based on nuanced semantic meanings, rather than just surface-level similarities. Furthermore, as we advance through the layers, the model gains access to more contextual information, further enhancing its ability to understand and process language.\subsubsection{Positional Embeddings:}
In here let's analyse a simple BERT encoder, our text sequence will be the word "recipe" repeated 20 times.\\[10pt]

\begin{figure}[htbp]
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/positional\_embeddings}
        \caption{Similarity score of our text}
        \label{similarityScore}
    \end{minipage}\hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/semi\_lua.png}
        \caption{Positional Embeddings of our text}
         \label{posEmb}
    \end{minipage}
\end{figure}

In Figure \ref{similarityScore} we notice that the diagonal elements have a similarity score of 1, which is unsurprising as they represent comparisons between the same token in the same position, across both sentences. As we traverse away from the diagonal, the similarity score decreases, indicating that the words are farther apart in position. The behaviour seen in the first row and column is due to the BERT model utilising sinusoidal functions for computing the positional embeddings creating the pattern seen in Figure \ref{similarityScore} which is similar to what "On Position Embedding in BERT" achieved~\cite{wang2021on}.

In Figure \ref{posEmb} we can also see a pattern that resembles the sinusoidal function which strengthens the conclusions above.
\subsubsection{Self Attention:} 
Here we will explore the differences between the \textbf{cross encoder} and \textbf{dual encoder} architectures.

In cross encoders we not only compute the self-attention, but we also compute the co-attention.
That is, we compute the attention between our input Query and the Key values, in order to force our model to relate both text sequences directly. This allows our model to grasp more detailed relations rather than just comparing the embeddings of each independently encoded sentence (also computed in the Query-Query relation, and Key-Key).
This is helpful because it allows our model to capture more intricate connections between the input texts. It can identify how the meaning of a token is influenced by the presence of another token in the other text.

\begin{figure}[htbp]
    \centering
    \begin{minipage}{0.50\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/full\_matrix.png}
        \caption{Self Attention Matrix}
        \label{selfAttention}
    \end{minipage}\hfill
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/self\_attention\_class.jpg}
        \caption{Self Attention Matrix Showed in Class}
         \label{selfAttentionClass}
    \end{minipage}
\end{figure}

As we can see in Figure \ref{selfAttention} we can clearly see the matrix separated into four squares, with each square corresponding to the self attention matrices we mentioned earlier. The matrix resembles the self attention matrix talked in class. We can see that Figure \ref{selfAttentionClass} resembles the matrix present in Figure \ref{selfAttention}.

As for the dual encoder both the sentences are encoded independently, with their similarities being computed later on, leading to a different results when compared to cross encoder. The advantage of this architecture compared to cross encoding is that the computational requirements aren't as demanding when compared to the cross encoders. 

\begin{figure}[htbp]
    \centering
    \begin{minipage}{0.50\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/dual\_key.png}
        \caption{Self Attention for Key}
         \label{dual1}
    \end{minipage}\hfill
    \begin{minipage}{0.50\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/dual\_query.png}
        \caption{Self Attention for Query}
         \label{dual2}
    \end{minipage}
\end{figure}

In the embeddings of Figures \ref{dual1} and \ref{dual2} the embeddings are calculated independently.
By passing the same two phrases to both types of encoders we will get different results based on the architecture chosen, let's explore why is that.\\[5pt]

\hspace{0.5cm} % Adjust the value as needed for the desired horizontal shift
\begin{minipage}{1.2\textwidth} % Adjust width as needed
\begin{dialogue}
\speak{Text1} "How to make pasta?"
\speak{Text2} "How much 1KG of pasta costs?"\\[5pt]
\end{dialogue}
\end{minipage}

When we perform dual encoding, we encode each phrase separately and then calculate the similarity score between them. Using the encoder described in~\cite{reimers-2019-sentence-bert}, we might obtain an approximate score of 0.72 for two phrases discussing pasta but in different contexts. However, with cross encoding using the same model, both phrases are encoded within the same context. Consequently, the embeddings take into account the context of both phrases, resulting in a lower similarity score (0.36). This outcome is reasonable as the phrases, although mentioning pasta, are discussing different topics.
\subsection{Dialog Manager}
Conversational AIs are now prevalent, found in applications ranging from ChatGPT and Siri to voice assistants in our cars. These technologies have a wide range of applications, and we plan to leverage them in our project. Utilizing the setup established in the previous sections, we will orchestrate a dialogue system that identifies user intents. This system will guide the user through finding a recipe and completing the selected recipe.
\subsubsection{Intent Detector}
We need to understand what the user wants. For that we utilize an intent detector in order to extract the intent from a natural language phrase given by the user. We used an adapted TWIZ intent detection model, trained on the TWIZ dataset ~\cite{intentdetect}. The intents extracted from the user input will be used in order to traverse our \textbf{state machine} and control the dialog with the user.
\subsubsection{Slot-Filling}
In addition to identifying the user's intent to determine the required action, it is essential to extract relevant information from their input. This process is known as slot-filling, where slots act as variables containing values necessary for informing the dialog manager's decisions. By recognising these variables from the user's prompt, we can effectively understand the prompt's context and identify the data needed for further processing.

We used the \textbf{roberta-base} model trained on question-answer pairs for slot-filling extraction~\cite{slot}. This method, known as Question-Answering, helps extract relevant information from text. By preparing a set of questions and feeding both these questions and the user input into the model, we can extract important variables for our search. For example, to identify the process, we use the following questions:

\hspace{2.5cm} % Adjust the value as needed for the desired horizontal shift
\begin{minipage}{1.2\textwidth} % Adjust width as needed
\begin{dialogue}
\speak{Q1} "What are we making or cooking?"
\speak{Q2} "What are the ingredients?"
\speak{Q3} "What is the cooking time?"
\speak{Q4} "How many servings?"
\speak{Q5} "What is the cooking or food style?"
\speak{Q6} "How easy, hard or difficult is it?"\\[5pt]
\end{dialogue}
\end{minipage}

By asking these questions to the model and feeding the user input we are able to retrieve the relevant variables/information from the prompt.
\subsubsection{State Machine}
To manage our dialog with the user and leverage the \textbf{intent detector} and \textbf{slot-filler}, we designed a state machine that governs the conversation flow. The \textbf{intents} serve as transitions between states, while the \textbf{variables} extracted by the \textbf{slot-filler} enable us to perform operations or searches when entering a specific state via a particular transition.

\begin{figure}[!htbp]
    \center
    \includegraphics[scale=0.15]{images/automata}
    \caption{State Machine}
    \label{autom}
\end{figure}
We have defined all possible state transitions and use the automaton in Figure \ref{autom} to guide the conversation. By employing a library~\cite{autolib} to construct this state machine, we automatically implement safeguards. If an undefined transition is encountered, a generic response is returned. Additionally, users have the option to exit the PlanLLM realm and reset to the beginning of our state machine. For an explanation of all the acronyms used in the transitions, please refer to the \texttt{StateMachineCB.py} file.
\newpage

Here is an example of the conversation with the agent:\\

\hspace{0.5cm} % Adjust the value as needed for the desired horizontal shift
\begin{minipage}{1.2\textwidth} % Adjust width as needed
\begin{dialogue}
\speak{ChatBot} Hiya!
\speak{User} I want a recipe with tomato and chicken 
\speak{ChatBot} I found these recipes that match what you asked for.\\
 Let me know which one you prefer:\\ 
 \texttt{IMAGE 1 - Burrito Cups} \texttt{IMAGE 2 - NAME 2} ...\\
 If you don't like these recipe ideas, ask me again for more.
\speak{User} I want the first one.
\speak{ChatBot} So you want to make: Burrito Cups correct?
\speak{User} Yes, I want that one.
\speak{ChatBot} Ready to dive in? Step 1: Heat a large skillet ...
\speak{User} Go to the next step.
\speak{ChatBot} Step 2: Mix together all the burrito ...\\
\speak{User} Actually I want to exit this recipe.
\speak{ChatBot} If you want to explore more recipes, just ask me!\\[5pt]
\end{dialogue}
\end{minipage}
The agent can guide the user to a recipe either through suggestions or by searching for what the user wants. Once a recipe is selected, PlanLLM assists the user step-by-step through the recipe, with the option to quit at any time and return to the start of the conversation.


\subsubsection{Guess the recipe game}
For our group-specific implementation, we chose to add a gamification element to our project. We implemented a guessing game where the user thinks of a recipe and provides an initial ingredient found in that recipe. The user then asks yes-or-no questions to help us identify the recipe they have in mind. This is similar to the Akinator game but tailored to the context of our project.

In order to do that we must add a new state in our state machine that will be responsible for processing the game as can be seen in Figure \ref{automz}.

\begin{figure}[!htbp]
    \center
    \includegraphics[scale=0.1]{images/aki}
    \caption{Guessing game state}
    \label{automz}
\end{figure}
To start the game, the user must type a prompt that includes the string \textbf{akinator}. This will signal that the user wants to play the Akinator game, and upon entering the appropriate state, the game will begin.

To implement the guessing game algorithm, we first construct an initial matrix where each column represents an ingredient and each row represents a recipe. Each cell in the matrix is either 0 or 1, indicating whether a certain ingredient is present in that recipe or not. We perform pre-processing by using embedding similarity for each ingredient to construct a JSON file (\texttt{similar\_ingredients.json}) containing all similar ingredients to a given one.

To guess the recipe, we prompt the user for ingredients, choosing the ingredient to ask based on the one with the highest entropy value. This helps narrow down options more effectively. We calculate the probability of each ingredient being in a certain recipe by counting the number of 1's in each row (indicating the presence of the ingredient in a recipe) and dividing it by the total number of ingredients.

Selecting the ingredient with the highest entropy value allows us to remove more rows from our matrix compared to any other ingredient (along with discarding the ingredient with the highest entropy). Consequently, this presents a maximization problem where our goal is to always maximize the entropy of the next question posed to the user. Figure \ref{akimat} provides a visual representation of the algorithm.

\begin{figure}[!htbp]
    \center
    \includegraphics[scale=0.15]{images/akinator}
    \caption{Guessing game state}
    \label{akimat}
\end{figure}

\section{Evaluation}
\subsection{Dataset Description}
The dataset contains a significant amount of recipe information, but not all of it is pertinent to our objectives. After careful examination, we have identified the essential data required for generating meaningful embeddings and providing accurate search results to users. Our focus has been on excluding irrelevant information and prioritizing specific details that enhance the user's search experience.

Following a thorough dataset analysis, we selectively filtered the essential data required for crafting our embeddings and information pertinent to users for search results. Our assessment highlighted the necessity for detailed information on certain aspects while deeming others less crucial.

During our process of embedding sentences and experimenting with various techniques, we encountered two significant problems. Firstly, the \textbf{displayText} of each ingredient contained excessive information, including opinions and irrelevant text. Secondly, some \textbf{displayText} entries were in languages other than English (e.g., Italian). This posed challenges, as searching for ingredients like Italian cheese would erroneously suggest a strong relationship due to language inconsistencies.

Therefore, to address these issues, we implemented two solutions. Firstly, we opted to utilize only the \textbf{ingredient} property of each ingredient. To accomplish this, we employed a model that extracts the ingredient from the \textbf{displayText} and populates the ingredient field in the JSON.

For that, we used the \url{https://github.com/strangetom/ingredient-parser}~\cite{ingredientParser} in order to extract the ingredients from a phrase. With this we solved the issue of having \textbf{null} values in these fields. However, the problem persisted with \textbf{textDisplay} being in another language, as the ingredient parser model~\cite{ingredientParser} simply returned the \textbf{displayText} as it was originally.

By utilizing a translator model~\cite{deeptranslate} before feeding the data to the ingredient parser~\cite{ingredientParser}, we successfully translated the \textbf{displayTexts} that were in other languages into English. This allowed us to apply the model to extract the ingredients and populate the missing fields.

These modifications to our original dataset resulted in a more comprehensive dataset. We created a new JSON file containing all of the original information along with the additional data. This file is accessible through OpenSearch, as it is stored without being indexed. Its sole purpose is to be retrievable from OpenSearch.

\subsection{PlanLLM Description}
Every step in our recipes have their respective embeddings encoded with CLIP, we generate these offline and store them locally for later consultation. Additionally, some steps have images associated with them, and we also store the embeddings of these images locally. When we're working within the context of a recipe, we retrieve the necessary similarities by computing the similarity score with CLIP using the inner product. We made the decision to handle this locally to avoid imposing unnecessary overhead on OpenSearch, as it seemed more efficient to manage this task outside of the OpenSearch environment.

As we engage in dialogue with PlanLLM, each API response triggers an update to our JSON file, which acts as our conduit for communication with PlanLLM. This ongoing updating ensures that our JSON file always reflects the latest context and information, ready to be utilised in subsequent conversations with PlanLLM. This dynamic process enables us to maintain coherence and continuity in our interactions with PlanLLM throughout the conversation.

\subsection{Baselines}
I don't know XD (palha?)
\subsection{Results analysis}
After implementing all the elements in the project pipeline we allow a user to communicate with a ChatBot in order to guide him from: selecting a recipe that the user wants, picking that recipe, guiding him trough that recipe until the end or even switching to another one if the user wants to do that.

\begin{figure}[!htbp]
    \center
    \includegraphics[scale=0.2]{images/prototype}
    \caption{Prototype of the ChatBot}
    \label{proto}
\end{figure}


\section{Critical discussion}
We've introduced a fully functional ChatBot with interactive features that leverage embedding similarity, both for text and image processing. Additionally, the implementation includes a dialog manager to coordinate the entire conversation. By incorporating gamification elements, the ChatBot offers more engaging and diverse interactions, enhancing the overall user experience.

Despite achieving excellent results in both search and dialogue, we encountered imperfections. Occasionally, depending on the image provided by the user, we may retrieve recipes that don't actually contain the specified ingredient due to similarities in color. To address this challenge, we had to acknowledge these limitations and devise workarounds. Some of our approaches include:

\begin{itemize}
	\item We translated certain ingredients to prevent unnecessary similarities;
	\item We implemented sum embeddings to extract additional information from our dataset;
	\item We adapted PlanLLM to enable navigation to a specific step based on the user's input regarding that step, or even if an image is provided as input;
\end{itemize}

Occasionally, the intent detector produced unexpected results, yielding some strange outcomes. Fine-tuning the model would be beneficial to enhance accuracy and mitigate these issues, ensuring more reliable results. Despite this, the intent extraction generally performs adequately for our requirements.

Creating effective questions for the slot-filler posed another challenge, as certain questions yielded better results than others. To address this, we conducted experimentation to identify the most suitable questions to use with the question-answering system, ensuring optimal performance of the slot-filler.

\subsection{Future Work}
For future enhancements, several areas could be further developed. Firstly, optimizing the intent detector to fully utilize its capabilities would enable its application in broader contexts. Secondly, exploring additional embedding options and conducting more queries to OpenSearch could improve the quality of results, allowing for better selection among existing options.

To further enhance the guessing game, an option is to train a dedicated model to predict the recipe a user is thinking of. This would entail gathering gameplay data from users and training the model based on this information. Such an approach diverges from our current method and holds the potential to significantly improve the game's performance and would be a different approach from what we have done.

\bibliographystyle{plain}
\bibliography{my_bib}

\end{document}
